{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9c7fd5b-99c0-4ec6-8c1a-bc724a8c1263",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20976841-f12e-4575-9e65-df53e3096ff0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import s3fs\n",
    "import numpy as np\n",
    "import fireducks.pandas as pd\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "from ml_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bfede60-9124-4c0f-a243-684e52391db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "warnings.simplefilter(\"ignore\")\n",
    "fs = s3fs.S3FileSystem(\n",
    "            client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"},\n",
    "            key=os.environ[\"Accesskey\"],\n",
    "            secret=os.environ[\"Secretkey\"],\n",
    "            token=os.environ[\"Token\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfefd20-d69c-46de-8d59-67096ab8b371",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad5e87e5-7b6a-4f1d-8fa1-d429f5cbdc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase_text</th>\n",
       "      <th>sol</th>\n",
       "      <th>clean_phrase_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>* Aider à la mise en place de l évènement Shar...</td>\n",
       "      <td>0</td>\n",
       "      <td>aider mise place évènemer shareplan envoi rapp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>* Comprendre le métier des achats * Comment or...</td>\n",
       "      <td>0</td>\n",
       "      <td>comprendre métier achat comment organiser appe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>* Fendre du bois en forêt au merlin manuelleme...</td>\n",
       "      <td>0</td>\n",
       "      <td>fendre boi forêt merlin manuellemer débarder b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2 jours au CDI , 1 jour en arts plastiques , 1...</td>\n",
       "      <td>0</td>\n",
       "      <td>2 jour cdi 1 jour art plastique 1 jour musiqu ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4 jours au sein du Bureau des affaires institu...</td>\n",
       "      <td>1</td>\n",
       "      <td>4 jour sein bureau affaire institutionnel fina...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with fs.open(\"elissamim/text_classification_men/data/stages-votes.json\", \"r\") as file:\n",
    "    df = pd.read_json(file)\n",
    "\n",
    "df = df.groupby(\"phrase_text\", as_index = False)[\"sol\"].apply(lambda x: x.mode().iloc[0])\n",
    "df[\"sol\"]=df[\"sol\"].apply(lambda x: 1 if x == \"ok\" else 0)\n",
    "df[\"clean_phrase_text\"] = df[\"phrase_text\"].apply(lambda x: nltk_text_preprocessing(x, True))\n",
    "df = df[df[\"clean_phrase_text\"] != \"\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4e3346-d25a-4e73-a41d-2b4eb7cd41aa",
   "metadata": {},
   "source": [
    "# Model selection (static embedding (sparse or dense) + classification algorithm) with grid search and cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c95cca3e-4f24-4689-99a4-5bd302a7a682",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"clean_phrase_text\"]\n",
    "y = df[\"sol\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "071f8ae2-9587-4608-a156-ba67eb9896c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Static embeddings:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Classification algorithms:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Classification algorithms:  20%|██        | 1/5 [00:01<00:06,  1.64s/it]\u001b[A\n",
      "Classification algorithms:  40%|████      | 2/5 [00:12<00:21,  7.29s/it]\u001b[A\n",
      "Classification algorithms:  60%|██████    | 3/5 [00:20<00:15,  7.57s/it]\u001b[A\n",
      "Classification algorithms:  80%|████████  | 4/5 [00:22<00:05,  5.10s/it]\u001b[A\n",
      "Classification algorithms: 100%|██████████| 5/5 [00:25<00:00,  5.12s/it]\u001b[A\n",
      "Static embeddings:  20%|██        | 1/5 [00:25<01:42, 25.60s/it]\n",
      "Classification algorithms:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Classification algorithms:  20%|██        | 1/5 [00:01<00:05,  1.41s/it]\u001b[A\n",
      "Classification algorithms:  40%|████      | 2/5 [00:12<00:21,  7.17s/it]\u001b[A\n",
      "Classification algorithms:  60%|██████    | 3/5 [00:20<00:15,  7.68s/it]\u001b[A\n",
      "Classification algorithms:  80%|████████  | 4/5 [00:22<00:05,  5.32s/it]\u001b[A\n",
      "Classification algorithms: 100%|██████████| 5/5 [00:35<00:00,  7.10s/it]\u001b[A\n",
      "Static embeddings:  40%|████      | 2/5 [01:01<01:34, 31.42s/it]\n",
      "Classification algorithms:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Classification algorithms:  20%|██        | 1/5 [00:01<00:07,  1.81s/it]\u001b[A\n",
      "Classification algorithms:  40%|████      | 2/5 [00:13<00:22,  7.42s/it]\u001b[A\n",
      "Classification algorithms:  60%|██████    | 3/5 [00:21<00:15,  7.79s/it]\u001b[A\n",
      "Classification algorithms:  80%|████████  | 4/5 [00:23<00:05,  5.39s/it]\u001b[A\n",
      "Classification algorithms: 100%|██████████| 5/5 [00:31<00:00,  6.36s/it]\u001b[A\n",
      "Static embeddings:  60%|██████    | 3/5 [01:32<01:03, 31.59s/it]\n",
      "Classification algorithms:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Classification algorithms:  20%|██        | 1/5 [00:22<01:31, 22.89s/it]\u001b[A\n",
      "Classification algorithms:  40%|████      | 2/5 [00:34<00:48, 16.20s/it]\u001b[A\n",
      "Classification algorithms:  60%|██████    | 3/5 [01:09<00:49, 24.77s/it]\u001b[A\n",
      "Classification algorithms: 100%|██████████| 5/5 [01:15<00:00, 15.16s/it]\u001b[A\n",
      "Static embeddings:  80%|████████  | 4/5 [02:48<00:49, 49.05s/it]\n",
      "Classification algorithms:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Classification algorithms:  20%|██        | 1/5 [00:15<01:01, 15.25s/it]\u001b[A\n",
      "Classification algorithms:  40%|████      | 2/5 [00:33<00:50, 16.77s/it]\u001b[A\n",
      "Classification algorithms:  60%|██████    | 3/5 [01:30<01:10, 35.15s/it]\u001b[A\n",
      "Classification algorithms: 100%|██████████| 5/5 [01:40<00:00, 20.10s/it]\u001b[A\n",
      "Static embeddings: 100%|██████████| 5/5 [04:29<00:00, 53.84s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts = [text.split() for text in X_train]\n",
    "word2vec_model = Word2Vec(sentences = tokenized_texts,\n",
    "                         vector_size = 100,\n",
    "                         window = 5,\n",
    "                         min_count = 1,\n",
    "                         workers = 4,\n",
    "                         seed = 42)\n",
    "fasttext_model = FastText(sentences = tokenized_texts,\n",
    "                         vector_size = 100,\n",
    "                         window = 5,\n",
    "                         min_count = 1,\n",
    "                         workers = 4,\n",
    "                         seed = 42)\n",
    "\n",
    "static_embedding_models = {\n",
    "    # Sparse embeddings\n",
    "    \"Bag of Words\":CountVectorizer(),\n",
    "    \"TF\":TfidfVectorizer(use_idf=False, norm = \"l1\"),\n",
    "    \"TF-IDF\":TfidfVectorizer(),\n",
    "    # Dense embeddings\n",
    "    \"Word2Vec\": MeanEmbeddingVectorizer(model=word2vec_model),\n",
    "    \"FastText\": MeanEmbeddingVectorizer(model=fasttext_model)\n",
    "}\n",
    "\n",
    "classification_models = {\n",
    "    \"Logistic Regression\":LogisticRegression(),\n",
    "    \"Random Forest\":RandomForestClassifier(),\n",
    "    \"Linear SVM\":SVC(kernel=\"linear\", probability=True),\n",
    "    \"Multinomial Naive Bayes\":MultinomialNB(),\n",
    "    \"XGBoost\":XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5ca4b7-c0c2-42ae-9415-d934ce326cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid_tfidf = {\n",
    "    \"tfidf__max_df\":[.7, .8, .9, 1],\n",
    "    \"tfidf__min_df\":[.001, .01, .1],\n",
    "    \"tfidf__norm\":[\"l1\", \"l2\", None],\n",
    "    \"tfidf__sublinear_tf\":[True, False],\n",
    "    \"tfidf__max_features\":[10, 100, 1000, 10000],\n",
    "    \"tfidf__ngram_range\":[(1,1), (1,2), (2,2)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93deb4a-83b1-4554-9260-e967f9dbec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_scores = {}\n",
    "\n",
    "for embedding_name, embedding_model in tqdm(static_embedding_models.items(),\n",
    "                                           desc=\"Static embeddings\"):\n",
    "\n",
    "    dict_scores[embedding_name] = {}\n",
    "    \n",
    "    for classification_name, classification_model in tqdm(classification_models.items(),\n",
    "                                                         desc=\"Classification algorithms\"):\n",
    "\n",
    "        # Multinomial NB is not suited for dense vectors\n",
    "        if embedding_name in [\"Word2Vec\", \"FastText\"] and classification_name == \"Multinomial Naive Bayes\":\n",
    "            continue\n",
    "\n",
    "        steps = [(\"feature_extraction\", embedding_model)]\n",
    "\n",
    "        # For Logistic Regression and Linear SVM, and for dense embeddings, add standardisation\n",
    "        if embedding_name in [\"Word2Vec\", \"FastText\"] and classification_name in [\"Logistic Regression\", \"Linear SVM\"]:\n",
    "            steps.append((\"standardisation\", StandardScaler()))\n",
    "\n",
    "        steps.append((\"classifier\", classification_model))\n",
    "\n",
    "        pipeline = Pipeline(steps)\n",
    "\n",
    "        # We compute scores using Grid Search on the parameter grid to do model selection with best\n",
    "        # hyperparameters\n",
    "\n",
    "        dict_scores[embedding_name][classification_name] = f\"{np.mean(scores):.3f} ± {np.std(scores):.3f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e8527d47-bd23-43a4-8d50-7ba0327a0701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Bag of Words': {'Linear SVM': '0.644 ± 0.035',\n",
      "                  'Logistic Regression': '0.686 ± 0.040',\n",
      "                  'Multinomial Naive Bayes': '0.674 ± 0.044',\n",
      "                  'Random Forest': '0.686 ± 0.031',\n",
      "                  'XGBoost': '0.680 ± 0.033'},\n",
      " 'FastText': {'Linear SVM': '0.603 ± 0.003',\n",
      "              'Logistic Regression': '0.597 ± 0.018',\n",
      "              'Random Forest': '0.567 ± 0.028',\n",
      "              'XGBoost': '0.551 ± 0.041'},\n",
      " 'TF': {'Linear SVM': '0.602 ± 0.007',\n",
      "        'Logistic Regression': '0.613 ± 0.020',\n",
      "        'Multinomial Naive Bayes': '0.601 ± 0.004',\n",
      "        'Random Forest': '0.673 ± 0.034',\n",
      "        'XGBoost': '0.680 ± 0.030'},\n",
      " 'TF-IDF': {'Linear SVM': '0.669 ± 0.035',\n",
      "            'Logistic Regression': '0.691 ± 0.030',\n",
      "            'Multinomial Naive Bayes': '0.672 ± 0.037',\n",
      "            'Random Forest': '0.690 ± 0.037',\n",
      "            'XGBoost': '0.677 ± 0.030'},\n",
      " 'Word2Vec': {'Linear SVM': '0.589 ± 0.051',\n",
      "              'Logistic Regression': '0.589 ± 0.047',\n",
      "              'Random Forest': '0.657 ± 0.030',\n",
      "              'XGBoost': '0.642 ± 0.038'}}\n"
     ]
    }
   ],
   "source": [
    "pprint(dict_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64680154-5512-4782-b346-5975736f0736",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = {\n",
    "    \"tfidf__max_df\":[.7, .8, .9, 1],\n",
    "    \"tfidf__min_df\":[.001, .01, .1],\n",
    "    \"tfidf__norm\":[\"l1\", \"l2\", None],\n",
    "    \"tfidf__sublinear_tf\":[True, False],\n",
    "    \"tfidf__max_features\":[10, 100, 1000, 10000],\n",
    "    \"tfidf__ngram_range\":[(1,1), (1,2), (2,2)],\n",
    "    \"logreg__C\":[.001, .01, .1, 1, 10, 100],\n",
    "    \"logreg__penalty\":[\"l2\"],\n",
    "    \"logreg__solver\":[\"lbfgs\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aac2c4d-831d-4174-a724-dd4e4d0478ca",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f1df5b07-8d1b-4276-9375-e3f87bb068fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logreg__C': 0.01, 'logreg__penalty': 'l2', 'logreg__solver': 'lbfgs', 'tfidf__max_df': 0.7, 'tfidf__max_features': 10000, 'tfidf__min_df': 0.001, 'tfidf__ngram_range': (1, 1), 'tfidf__norm': None, 'tfidf__sublinear_tf': False}\n",
      "0.7028301886792453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../models/tfidf_logreg_model.joblib']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"logreg\", LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    params_grid,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fdec1671-1a14-483c-b316-73fb5bac4af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6578947368421053\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.83      0.75       166\n",
      "           1       0.57      0.37      0.45       100\n",
      "\n",
      "    accuracy                           0.66       266\n",
      "   macro avg       0.63      0.60      0.60       266\n",
      "weighted avg       0.64      0.66      0.64       266\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[138,  28],\n",
       "       [ 63,  37]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "478278c0-3a30-47f2-a4df-84dfe4aad081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/tfidf_logreg_model.joblib']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(grid_search.best_estimator_,\n",
    "            \"../models/tfidf_logreg_model.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
