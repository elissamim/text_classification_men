{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9c7fd5b-99c0-4ec6-8c1a-bc724a8c1263",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20976841-f12e-4575-9e65-df53e3096ff0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package punkt to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fr-core-news-sm\n",
      "Successfully installed fr-core-news-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import s3fs\n",
    "import numpy as np\n",
    "import fireducks.pandas as pd\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import (\n",
    "                                    cross_val_score,\n",
    "                                    RandomizedSearchCV,\n",
    "                                    GridSearchCV, \n",
    "                                    train_test_split,\n",
    "                                    StratifiedKFold\n",
    "                                    )\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "from ml_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bfede60-9124-4c0f-a243-684e52391db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "warnings.simplefilter(\"ignore\")\n",
    "fs = s3fs.S3FileSystem(\n",
    "            client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"},\n",
    "            key=os.environ[\"Accesskey\"],\n",
    "            secret=os.environ[\"Secretkey\"],\n",
    "            token=os.environ[\"Token\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfefd20-d69c-46de-8d59-67096ab8b371",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad5e87e5-7b6a-4f1d-8fa1-d429f5cbdc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase_text</th>\n",
       "      <th>sol</th>\n",
       "      <th>clean_phrase_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>* Aider à la mise en place de l évènement Shar...</td>\n",
       "      <td>0</td>\n",
       "      <td>aider mise place évènemer shareplan envoi rapp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>* Comprendre le métier des achats * Comment or...</td>\n",
       "      <td>0</td>\n",
       "      <td>comprendre métier achat comment organiser appe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>* Fendre du bois en forêt au merlin manuelleme...</td>\n",
       "      <td>0</td>\n",
       "      <td>fendre boi forêt merlin manuellemer débarder b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2 jours au CDI , 1 jour en arts plastiques , 1...</td>\n",
       "      <td>0</td>\n",
       "      <td>2 jour cdi 1 jour art plastique 1 jour musiqu ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4 jours au sein du Bureau des affaires institu...</td>\n",
       "      <td>1</td>\n",
       "      <td>4 jour sein bureau affaire institutionnel fina...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with fs.open(\"elissamim/text_classification_men/data/stages-votes.json\", \"r\") as file:\n",
    "    df = pd.read_json(file)\n",
    "\n",
    "df = df.groupby(\"phrase_text\", as_index = False)[\"sol\"].apply(lambda x: x.mode().iloc[0])\n",
    "df[\"sol\"]=df[\"sol\"].apply(lambda x: 1 if x == \"ok\" else 0)\n",
    "df[\"clean_phrase_text\"] = df[\"phrase_text\"].apply(lambda x: nltk_text_preprocessing(x, True))\n",
    "df = df[df[\"clean_phrase_text\"] != \"\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4e3346-d25a-4e73-a41d-2b4eb7cd41aa",
   "metadata": {},
   "source": [
    "# Model selection (static embedding (sparse or dense) + classification algorithm) with grid search and cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c95cca3e-4f24-4689-99a4-5bd302a7a682",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"clean_phrase_text\"]\n",
    "y = df[\"sol\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "071f8ae2-9587-4608-a156-ba67eb9896c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_texts = [text.split() for text in X_train]\n",
    "word2vec_model = Word2Vec(sentences = tokenized_texts,\n",
    "                         vector_size = 100,\n",
    "                         window = 5,\n",
    "                         min_count = 1,\n",
    "                         workers = 4,\n",
    "                         seed = 42)\n",
    "fasttext_model = FastText(sentences = tokenized_texts,\n",
    "                         vector_size = 100,\n",
    "                         window = 5,\n",
    "                         min_count = 1,\n",
    "                         workers = 4,\n",
    "                         seed = 42)\n",
    "\n",
    "static_embedding_models = {\n",
    "    # Sparse embeddings\n",
    "    \"Bag of Words\":CountVectorizer(),\n",
    "    \"TF\":TfidfVectorizer(use_idf=False),\n",
    "    \"TF-IDF\":TfidfVectorizer(),\n",
    "    # Dense embeddings\n",
    "    \"Word2Vec\": MeanEmbeddingVectorizer(model=word2vec_model),\n",
    "    \"FastText\": MeanEmbeddingVectorizer(model=fasttext_model)\n",
    "}\n",
    "\n",
    "classification_models = {\n",
    "    \"Logistic Regression\":LogisticRegression(),\n",
    "    \"Random Forest\":RandomForestClassifier(),\n",
    "    \"Linear SVM\":SVC(kernel=\"linear\", probability=True),\n",
    "    \"Multinomial Naive Bayes\":MultinomialNB(),\n",
    "    \"XGBoost\":XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a5ca4b7-c0c2-42ae-9415-d934ce326cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = {\n",
    "    \"Bag of Words\" : {\n",
    "        \"feature_extraction__ngram_range\":[(1,1), (1,2), (2,2)],\n",
    "        \"feature_extraction__max_df\":[.7, .9, 1],\n",
    "        \"feature_extraction__min_df\":[.001, .01, .1],\n",
    "        \"feature_extraction__max_features\":[10, 100, 1000, 10000],\n",
    "        \"feature_extraction__binary\":[True, False]\n",
    "    },\n",
    "    \"TF\" : {\n",
    "        \"feature_extraction__max_df\":[.7, .8, .9, 1],\n",
    "        \"feature_extraction__min_df\":[.001, .01, .1],\n",
    "        \"feature_extraction__norm\":[\"l1\", \"l2\", None],\n",
    "        \"feature_extraction__sublinear_tf\":[True, False],\n",
    "        \"feature_extraction__max_features\":[10, 100, 1000, 10000],\n",
    "        \"feature_extraction__ngram_range\":[(1,1), (1,2), (2,2)]\n",
    "    },\n",
    "    \"TF-IDF\" : {\n",
    "        \"feature_extraction__max_df\":[.7, .8, .9, 1],\n",
    "        \"feature_extraction__min_df\":[.001, .01, .1],\n",
    "        \"feature_extraction__norm\":[\"l1\", \"l2\", None],\n",
    "        \"feature_extraction__sublinear_tf\":[True, False],\n",
    "        \"feature_extraction__max_features\":[10, 100, 1000, 10000],\n",
    "        \"feature_extraction__ngram_range\":[(1,1), (1,2), (2,2)]\n",
    "    },\n",
    "    \"Logistic Regression\":{\n",
    "        \"classifier__C\":[.001, .01, .1, 1, 10, 100],\n",
    "        \"classifier__penalty\":[\"l2\"],\n",
    "        \"classifier__solver\":[\"lbfgs\"]\n",
    "    },\n",
    "    \"Random Forest\":{\n",
    "        \"classifier__n_estimators\":[100,300,500],\n",
    "        \"classifier__max_depth\":[None, 10, 20, 50],\n",
    "        \"classifier__min_samples_split\": [2, 5, 10],\n",
    "        \"classifier__min_samples_leaf\":[1, 2, 5, 10],\n",
    "        \"classifier__max_features\":[\"sqrt\", \"log2\", None],\n",
    "        \"classifier__bootstrap\":[True, False]\n",
    "    },\n",
    "    \"Linear SVM\":{\n",
    "        \"classifier__C\":[.001, .01, .1, 1, 10, 100]\n",
    "    },\n",
    "    \"Multinomial Naive Bayes\":{\n",
    "        \"classifier__alpha\":[.0001, .001, .01, .1, 1, 10],\n",
    "        \"classifier__fit_prior\":[True, False]\n",
    "    },\n",
    "    \"XGBoost\":{\n",
    "        \"classifier__n_estimators\":[100, 200, 300, 500],\n",
    "        \"classifier__max_depth\":[3, 5, 7, 10],\n",
    "        \"classifier__learning_rate\":[.01, .05, .1, .3],\n",
    "        \"classifier__subsample\":[.5, .7, .8, 1],\n",
    "        \"classifier__colsample_bytree\":[.5, .7, .8, 1],\n",
    "        \"classifier__gamma\":[0, .1, .5, 1],\n",
    "        \"classifier__reg_alpha\":[0, .01, .1, 1],\n",
    "        \"classifier__reg_lambda\":[.1, 1, 10]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f93deb4a-83b1-4554-9260-e967f9dbec38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Static embeddings:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Classification algorithms:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Static embeddings:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "/opt/conda/lib/python3.12/site-packages/joblib/_multiprocessing_helpers.py:46: UserWarning: [Errno 28] No space left on device.  joblib will operate in serial mode\n",
      "  warnings.warn('%s.  joblib will operate in serial mode' % (e,))\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     33\u001b[39m selected_params_grid = {\n\u001b[32m     34\u001b[39m     **params_grid.get(embedding_name, {}),\n\u001b[32m     35\u001b[39m     **params_grid.get(classification_name, {})\n\u001b[32m     36\u001b[39m }\n\u001b[32m     38\u001b[39m grid_search = RandomizedSearchCV(\n\u001b[32m     39\u001b[39m     pipeline,\n\u001b[32m     40\u001b[39m     param_distributions=selected_params_grid,\n\u001b[32m   (...)\u001b[39m\u001b[32m     45\u001b[39m     n_iter = \u001b[32m20\u001b[39m\n\u001b[32m     46\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m score = grid_search.best_score_\n\u001b[32m     51\u001b[39m dict_params[embedding_name][classification_name] = grid_search.best_params_\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/sklearn/model_selection/_search.py:952\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    940\u001b[39m fit_and_score_kwargs = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m    941\u001b[39m     scorer=scorers,\n\u001b[32m    942\u001b[39m     fit_params=routed_params.estimator.fit,\n\u001b[32m   (...)\u001b[39m\u001b[32m    949\u001b[39m     verbose=\u001b[38;5;28mself\u001b[39m.verbose,\n\u001b[32m    950\u001b[39m )\n\u001b[32m    951\u001b[39m results = {}\n\u001b[32m--> \u001b[39m\u001b[32m952\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parallel:\n\u001b[32m    953\u001b[39m     all_candidate_params = []\n\u001b[32m    954\u001b[39m     all_out = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/joblib/parallel.py:1347\u001b[39m, in \u001b[36mParallel.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1345\u001b[39m \u001b[38;5;28mself\u001b[39m._managed_backend = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28mself\u001b[39m._calling = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1347\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1348\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/joblib/parallel.py:1359\u001b[39m, in \u001b[36mParallel._initialize_backend\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1357\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Build a process or thread pool and return the number of workers\"\"\"\u001b[39;00m\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1359\u001b[39m     n_jobs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfigure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backend_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1361\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.supports_timeout:\n\u001b[32m   1362\u001b[39m         warnings.warn(\n\u001b[32m   1363\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mThe backend class \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[33m does not support timeout. \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1364\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou have set \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtimeout=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m in Parallel but \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1365\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mthe \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m'\u001b[39m\u001b[33m parameter will not be used.\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   1366\u001b[39m                 \u001b[38;5;28mself\u001b[39m._backend.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m,\n\u001b[32m   1367\u001b[39m                 \u001b[38;5;28mself\u001b[39m.timeout))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/joblib/_parallel_backends.py:538\u001b[39m, in \u001b[36mLokyBackend.configure\u001b[39m\u001b[34m(self, n_jobs, parallel, prefer, require, idle_worker_timeout, **memmappingexecutor_args)\u001b[39m\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m    535\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m FallbackToBackend(\n\u001b[32m    536\u001b[39m         SequentialBackend(nesting_level=\u001b[38;5;28mself\u001b[39m.nesting_level))\n\u001b[32m--> \u001b[39m\u001b[32m538\u001b[39m \u001b[38;5;28mself\u001b[39m._workers = \u001b[43mget_memmapping_executor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43midle_worker_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_worker_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparallel\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmemmappingexecutor_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[38;5;28mself\u001b[39m.parallel = parallel\n\u001b[32m    543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m n_jobs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/joblib/executor.py:20\u001b[39m, in \u001b[36mget_memmapping_executor\u001b[39m\u001b[34m(n_jobs, **kwargs)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_memmapping_executor\u001b[39m(n_jobs, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMemmappingExecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_memmapping_executor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/joblib/executor.py:52\u001b[39m, in \u001b[36mMemmappingExecutor.get_memmapping_executor\u001b[39m\u001b[34m(cls, n_jobs, timeout, initializer, initargs, env, temp_folder, context_id, **backend_args)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# reducers access the temporary folder in which to store temporary\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# pickles through a call to manager.resolve_temp_folder_name. resolving\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# the folder name dynamically is useful to use different folders across\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# calls of a same reusable executor\u001b[39;00m\n\u001b[32m     48\u001b[39m job_reducers, result_reducers = get_memmapping_reducers(\n\u001b[32m     49\u001b[39m     unlink_on_gc_collect=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     50\u001b[39m     temp_folder_resolver=manager.resolve_temp_folder_name,\n\u001b[32m     51\u001b[39m     **backend_args)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m _executor, executor_is_reused = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_reusable_executor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_reducers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjob_reducers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_reducers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresult_reducers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreuse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreuse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitializer\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[43menv\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m executor_is_reused:\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# Only set a _temp_folder_manager for new executors. Reused\u001b[39;00m\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# executors already have a _temporary_folder_manager that must not\u001b[39;00m\n\u001b[32m     61\u001b[39m     \u001b[38;5;66;03m# be re-assigned like that because it is referenced in various\u001b[39;00m\n\u001b[32m     62\u001b[39m     \u001b[38;5;66;03m# places in the reducing machinery of the executor.\u001b[39;00m\n\u001b[32m     63\u001b[39m     _executor._temp_folder_manager = manager\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/joblib/externals/loky/reusable_executor.py:182\u001b[39m, in \u001b[36m_ReusablePoolExecutor.get_reusable_executor\u001b[39m\u001b[34m(cls, max_workers, context, timeout, kill_workers, reuse, job_reducers, result_reducers, initializer, initargs, env)\u001b[39m\n\u001b[32m    180\u001b[39m     executor_id = _get_next_executor_id()\n\u001b[32m    181\u001b[39m     _executor_kwargs = kwargs\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     _executor = executor = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_executor_lock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutor_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    189\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m reuse == \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/joblib/externals/loky/reusable_executor.py:118\u001b[39m, in \u001b[36m_ReusablePoolExecutor.__init__\u001b[39m\u001b[34m(self, submit_resize_lock, max_workers, context, timeout, executor_id, job_reducers, result_reducers, initializer, initargs, env)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    106\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    107\u001b[39m     submit_resize_lock,\n\u001b[32m   (...)\u001b[39m\u001b[32m    116\u001b[39m     env=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    117\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjob_reducers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjob_reducers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresult_reducers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresult_reducers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m        \u001b[49m\u001b[43minitializer\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m        \u001b[49m\u001b[43minitargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28mself\u001b[39m.executor_id = executor_id\n\u001b[32m    129\u001b[39m     \u001b[38;5;28mself\u001b[39m._submit_resize_lock = submit_resize_lock\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:1110\u001b[39m, in \u001b[36mProcessPoolExecutor.__init__\u001b[39m\u001b[34m(self, max_workers, job_reducers, result_reducers, timeout, context, initializer, initargs, env)\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;28mself\u001b[39m._running_work_items = []\n\u001b[32m   1109\u001b[39m \u001b[38;5;28mself\u001b[39m._work_ids = queue.Queue()\n\u001b[32m-> \u001b[39m\u001b[32m1110\u001b[39m \u001b[38;5;28mself\u001b[39m._processes_management_lock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[38;5;28mself\u001b[39m._executor_manager_thread = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28mself\u001b[39m._shutdown_lock = threading.Lock()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/joblib/externals/loky/backend/context.py:335\u001b[39m, in \u001b[36mLokyContext.Lock\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns a lock object\"\"\"\u001b[39;00m\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01msynchronize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Lock\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/joblib/externals/loky/backend/synchronize.py:192\u001b[39m, in \u001b[36mLock.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSEMAPHORE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/joblib/externals/loky/backend/synchronize.py:72\u001b[39m, in \u001b[36mSemLock.__init__\u001b[39m\u001b[34m(self, kind, value, maxvalue, name)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m100\u001b[39m):\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m         \u001b[38;5;28mself\u001b[39m._semlock = \u001b[43m_SemLock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSemLock\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_make_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlink_now\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "dict_scores, dict_params = {}, {}\n",
    "\n",
    "for embedding_name, embedding_model in tqdm(static_embedding_models.items(),\n",
    "                                           desc=\"Static embeddings\"):\n",
    "\n",
    "    dict_scores[embedding_name], dict_params[embedding_name] = {}, {}\n",
    "    \n",
    "    for classification_name, classification_model in tqdm(classification_models.items(),\n",
    "                                                         desc=\"Classification algorithms\"):\n",
    "\n",
    "        # Multinomial NB is not suited for dense vectors\n",
    "        if embedding_name in [\"Word2Vec\", \"FastText\"] and classification_name == \"Multinomial Naive Bayes\":\n",
    "            continue\n",
    "\n",
    "        steps = [(\"feature_extraction\", embedding_model)]\n",
    "\n",
    "        # For Logistic Regression and Linear SVM, and for dense embeddings, add standardisation\n",
    "        if embedding_name in [\"Word2Vec\", \"FastText\"] and classification_name in [\"Logistic Regression\", \"Linear SVM\"]:\n",
    "            steps.append((\"standardisation\", StandardScaler()))\n",
    "\n",
    "        steps.append((\"classifier\", classification_model))\n",
    "\n",
    "        pipeline = Pipeline(steps)\n",
    "\n",
    "        # We compute scores using Grid Search on the parameter grid to do model selection with best\n",
    "        # hyperparameters\n",
    "        cv = StratifiedKFold(n_splits=5, \n",
    "                                 shuffle = True, \n",
    "                                 random_state=42)\n",
    "        \n",
    "        if ((embedding_name in params_grid) or (classification_name in params_grid)) and (embedding_name != \"FastText\"):\n",
    "\n",
    "            selected_params_grid = {\n",
    "                **params_grid.get(embedding_name, {}),\n",
    "                **params_grid.get(classification_name, {})\n",
    "            }\n",
    "\n",
    "            grid_search = RandomizedSearchCV(\n",
    "                pipeline,\n",
    "                param_distributions=selected_params_grid,\n",
    "                cv=cv,\n",
    "                scoring=\"accuracy\",\n",
    "                n_jobs=-1,\n",
    "                verbose=0,\n",
    "                n_iter = 20\n",
    "            )\n",
    "\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            \n",
    "            score = grid_search.best_score_\n",
    "            dict_params[embedding_name][classification_name] = grid_search.best_params_\n",
    "\n",
    "        else:\n",
    "\n",
    "            scores = cross_val_score(\n",
    "                pipeline,\n",
    "                X_train, \n",
    "                y_train,\n",
    "                cv=cv,\n",
    "                scoring=\"accuracy\"\n",
    "            )\n",
    "\n",
    "            score = np.mean(scores)\n",
    "            \n",
    "        dict_scores[embedding_name][classification_name] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8527d47-bd23-43a4-8d50-7ba0327a0701",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(dict_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aac2c4d-831d-4174-a724-dd4e4d0478ca",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1df5b07-8d1b-4276-9375-e3f87bb068fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"logreg\", LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    params_grid,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdec1671-1a14-483c-b316-73fb5bac4af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478278c0-3a30-47f2-a4df-84dfe4aad081",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(grid_search.best_estimator_,\n",
    "            \"../models/tfidf_logreg_model.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
