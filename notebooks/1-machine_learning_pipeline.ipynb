{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9c7fd5b-99c0-4ec6-8c1a-bc724a8c1263",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20976841-f12e-4575-9e65-df53e3096ff0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package punkt to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import s3fs\n",
    "import numpy as np\n",
    "import fireducks.pandas as pd\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import (\n",
    "                                    cross_val_score, \n",
    "                                    GridSearchCV, \n",
    "                                    train_test_split,\n",
    "                                    StratifiedKFold\n",
    "                                    )\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "from ml_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bfede60-9124-4c0f-a243-684e52391db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "warnings.simplefilter(\"ignore\")\n",
    "fs = s3fs.S3FileSystem(\n",
    "            client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"},\n",
    "            key=os.environ[\"Accesskey\"],\n",
    "            secret=os.environ[\"Secretkey\"],\n",
    "            token=os.environ[\"Token\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfefd20-d69c-46de-8d59-67096ab8b371",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad5e87e5-7b6a-4f1d-8fa1-d429f5cbdc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase_text</th>\n",
       "      <th>sol</th>\n",
       "      <th>clean_phrase_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>* Aider à la mise en place de l évènement Shar...</td>\n",
       "      <td>0</td>\n",
       "      <td>aider mise place évènemer shareplan envoi rapp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>* Comprendre le métier des achats * Comment or...</td>\n",
       "      <td>0</td>\n",
       "      <td>comprendre métier achat comment organiser appe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>* Fendre du bois en forêt au merlin manuelleme...</td>\n",
       "      <td>0</td>\n",
       "      <td>fendre boi forêt merlin manuellemer débarder b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2 jours au CDI , 1 jour en arts plastiques , 1...</td>\n",
       "      <td>0</td>\n",
       "      <td>2 jour cdi 1 jour art plastique 1 jour musiqu ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4 jours au sein du Bureau des affaires institu...</td>\n",
       "      <td>1</td>\n",
       "      <td>4 jour sein bureau affaire institutionnel fina...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with fs.open(\"elissamim/text_classification_men/data/stages-votes.json\", \"r\") as file:\n",
    "    df = pd.read_json(file)\n",
    "\n",
    "df = df.groupby(\"phrase_text\", as_index = False)[\"sol\"].apply(lambda x: x.mode().iloc[0])\n",
    "df[\"sol\"]=df[\"sol\"].apply(lambda x: 1 if x == \"ok\" else 0)\n",
    "df[\"clean_phrase_text\"] = df[\"phrase_text\"].apply(lambda x: nltk_text_preprocessing(x, True))\n",
    "df = df[df[\"clean_phrase_text\"] != \"\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4e3346-d25a-4e73-a41d-2b4eb7cd41aa",
   "metadata": {},
   "source": [
    "# Model selection (static embedding (sparse or dense) + classification algorithm) with grid search and cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c95cca3e-4f24-4689-99a4-5bd302a7a682",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"clean_phrase_text\"]\n",
    "y = df[\"sol\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "071f8ae2-9587-4608-a156-ba67eb9896c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_texts = [text.split() for text in X_train]\n",
    "word2vec_model = Word2Vec(sentences = tokenized_texts,\n",
    "                         vector_size = 100,\n",
    "                         window = 5,\n",
    "                         min_count = 1,\n",
    "                         workers = 4,\n",
    "                         seed = 42)\n",
    "fasttext_model = FastText(sentences = tokenized_texts,\n",
    "                         vector_size = 100,\n",
    "                         window = 5,\n",
    "                         min_count = 1,\n",
    "                         workers = 4,\n",
    "                         seed = 42)\n",
    "\n",
    "static_embedding_models = {\n",
    "    # Sparse embeddings\n",
    "    \"Bag of Words\":CountVectorizer(),\n",
    "    \"TF\":TfidfVectorizer(use_idf=False),\n",
    "    \"TF-IDF\":TfidfVectorizer(),\n",
    "    # Dense embeddings\n",
    "    \"Word2Vec\": MeanEmbeddingVectorizer(model=word2vec_model),\n",
    "    \"FastText\": MeanEmbeddingVectorizer(model=fasttext_model)\n",
    "}\n",
    "\n",
    "classification_models = {\n",
    "    \"Logistic Regression\":LogisticRegression(),\n",
    "    \"Random Forest\":RandomForestClassifier(),\n",
    "    \"Linear SVM\":SVC(kernel=\"linear\", probability=True),\n",
    "    \"Multinomial Naive Bayes\":MultinomialNB(),\n",
    "    \"XGBoost\":XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a5ca4b7-c0c2-42ae-9415-d934ce326cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = {\n",
    "    \"Bag of Words\" : {\n",
    "        \"feature_extraction__ngram_range\":[(1,1), (1,2), (2,2)],\n",
    "        \"feature_extraction__max_df\":[.7, .9, 1],\n",
    "        \"feature_extraction__min_df\":[.001, .01, .1],\n",
    "        \"feature_extraction__max_features\":[10, 100, 1000, 10000],\n",
    "        \"feature_extraction__binary\":[True, False]\n",
    "    },\n",
    "    \"TF\" : {\n",
    "        \"feature_extraction__max_df\":[.7, .8, .9, 1],\n",
    "        \"feature_extraction__min_df\":[.001, .01, .1],\n",
    "        \"feature_extraction__norm\":[\"l1\", \"l2\", None],\n",
    "        \"feature_extraction__sublinear_tf\":[True, False],\n",
    "        \"feature_extraction__max_features\":[10, 100, 1000, 10000],\n",
    "        \"feature_extraction__ngram_range\":[(1,1), (1,2), (2,2)]\n",
    "    },\n",
    "    \"TF-IDF\" : {\n",
    "        \"feature_extraction__max_df\":[.7, .8, .9, 1],\n",
    "        \"feature_extraction__min_df\":[.001, .01, .1],\n",
    "        \"feature_extraction__norm\":[\"l1\", \"l2\", None],\n",
    "        \"feature_extraction__sublinear_tf\":[True, False],\n",
    "        \"feature_extraction__max_features\":[10, 100, 1000, 10000],\n",
    "        \"feature_extraction__ngram_range\":[(1,1), (1,2), (2,2)]\n",
    "    },\n",
    "    \"Logistic Regression\":{\n",
    "        \"classifier__C\":[.001, .01, .1, 1, 10, 100],\n",
    "        \"classifier__penalty\":[\"l2\"],\n",
    "        \"classifier__solver\":[\"lbfgs\"]\n",
    "    },\n",
    "    \"Random Forest\":{\n",
    "        \"classifier__n_estimators\":[100,300,500],\n",
    "        \"classifier__max_depth\":[None, 10, 20, 50],\n",
    "        \"classifier__min_samples_split\": [2, 5, 10],\n",
    "        \"classifier__min_samples_leaf\":[1, 2, 5, 10],\n",
    "        \"classifier__max_features\":[\"sqrt\", \"log2\", None],\n",
    "        \"classifier__bootstrap\":[True, False]\n",
    "    },\n",
    "    \"Linear SVM\":{\n",
    "        \"classifier__C\":[.001, .01, .1, 1, 10, 100]\n",
    "    },\n",
    "    \"Multinomial Naive Bayes\":{\n",
    "        \"classifier__alpha\":[.0001, .001, .01, .1, 1, 10],\n",
    "        \"classifier__fit_prior\":[True, False]\n",
    "    },\n",
    "    \"XGBoost\":{\n",
    "        \"classifier__n_estimators\":[100, 200, 300, 500],\n",
    "        \"classifier__max_depth\":[3, 5, 7, 10],\n",
    "        \"classifier__learning_rate\":[.01, .05, .1, .3],\n",
    "        \"classifier__subsample\":[.5, .7, .8, 1],\n",
    "        \"classifier__colsample_bytree\":[.5, .7, .8, 1],\n",
    "        \"classifier__gamma\":[0, .1, .5, 1],\n",
    "        \"classifier__reg_alpha\":[0, .01, .1, 1],\n",
    "        \"classifier__reg_lambda\":[.1, 1, 10]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93deb4a-83b1-4554-9260-e967f9dbec38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Static embeddings:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Classification algorithms:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "dict_scores, dict_params = {}, {}\n",
    "\n",
    "for embedding_name, embedding_model in tqdm(static_embedding_models.items(),\n",
    "                                           desc=\"Static embeddings\"):\n",
    "\n",
    "    dict_scores[embedding_name], dict_params[embedding_name] = {}, {}\n",
    "    \n",
    "    for classification_name, classification_model in tqdm(classification_models.items(),\n",
    "                                                         desc=\"Classification algorithms\"):\n",
    "\n",
    "        # Multinomial NB is not suited for dense vectors\n",
    "        if embedding_name in [\"Word2Vec\", \"FastText\"] and classification_name == \"Multinomial Naive Bayes\":\n",
    "            continue\n",
    "\n",
    "        steps = [(\"feature_extraction\", embedding_model)]\n",
    "\n",
    "        # For Logistic Regression and Linear SVM, and for dense embeddings, add standardisation\n",
    "        if embedding_name in [\"Word2Vec\", \"FastText\"] and classification_name in [\"Logistic Regression\", \"Linear SVM\"]:\n",
    "            steps.append((\"standardisation\", StandardScaler()))\n",
    "\n",
    "        steps.append((\"classifier\", classification_model))\n",
    "\n",
    "        pipeline = Pipeline(steps)\n",
    "\n",
    "        # We compute scores using Grid Search on the parameter grid to do model selection with best\n",
    "        # hyperparameters\n",
    "        cv = StratifiedKFold(n_splits=5, \n",
    "                                 shuffle = True, \n",
    "                                 random_state=42)\n",
    "        \n",
    "        if (embedding_name in params_grid) or (classification_name in params_grid):\n",
    "\n",
    "            selected_params_grid = {\n",
    "                **params_grid.get(embedding_name, {}),\n",
    "                **params_grid.get(classification_name, {})\n",
    "            }\n",
    "            \n",
    "            grid_search = GridSearchCV(\n",
    "                pipeline,\n",
    "                param_grid=selected_params_grid,\n",
    "                cv=cv,\n",
    "                scoring=\"accuracy\",\n",
    "                n_jobs=-1,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            grid_search.fit(X_train, y_train)\n",
    "            \n",
    "            score = grid_search.best_score_\n",
    "            dict_params[embedding_name][classification_name] = grid_search.best_params_\n",
    "\n",
    "        else:\n",
    "\n",
    "            scores = cross_val_score(\n",
    "                pipeline,\n",
    "                X_train, \n",
    "                y_train,\n",
    "                cv=cv,\n",
    "                scoring=\"accuracy\"\n",
    "            )\n",
    "\n",
    "            score = np.mean(scores)\n",
    "            \n",
    "        dict_scores[embedding_name][classification_name] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8527d47-bd23-43a4-8d50-7ba0327a0701",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(dict_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aac2c4d-831d-4174-a724-dd4e4d0478ca",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f1df5b07-8d1b-4276-9375-e3f87bb068fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logreg__C': 0.01, 'logreg__penalty': 'l2', 'logreg__solver': 'lbfgs', 'tfidf__max_df': 0.7, 'tfidf__max_features': 10000, 'tfidf__min_df': 0.001, 'tfidf__ngram_range': (1, 1), 'tfidf__norm': None, 'tfidf__sublinear_tf': False}\n",
      "0.7028301886792453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../models/tfidf_logreg_model.joblib']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"logreg\", LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    params_grid,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fdec1671-1a14-483c-b316-73fb5bac4af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6578947368421053\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.83      0.75       166\n",
      "           1       0.57      0.37      0.45       100\n",
      "\n",
      "    accuracy                           0.66       266\n",
      "   macro avg       0.63      0.60      0.60       266\n",
      "weighted avg       0.64      0.66      0.64       266\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[138,  28],\n",
       "       [ 63,  37]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "478278c0-3a30-47f2-a4df-84dfe4aad081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/tfidf_logreg_model.joblib']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(grid_search.best_estimator_,\n",
    "            \"../models/tfidf_logreg_model.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
