{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9c7fd5b-99c0-4ec6-8c1a-bc724a8c1263",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20976841-f12e-4575-9e65-df53e3096ff0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package punkt to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import s3fs\n",
    "import numpy as np\n",
    "import fireducks.pandas as pd\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "from ml_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bfede60-9124-4c0f-a243-684e52391db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "warnings.simplefilter(\"ignore\")\n",
    "fs = s3fs.S3FileSystem(\n",
    "            client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"},\n",
    "            key=os.environ[\"Accesskey\"],\n",
    "            secret=os.environ[\"Secretkey\"],\n",
    "            token=os.environ[\"Token\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad5e87e5-7b6a-4f1d-8fa1-d429f5cbdc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase_text</th>\n",
       "      <th>sol</th>\n",
       "      <th>clean_phrase_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>* Aider à la mise en place de l évènement Shar...</td>\n",
       "      <td>0</td>\n",
       "      <td>aider mise place évènemer shareplan envoi rapp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>* Comprendre le métier des achats * Comment or...</td>\n",
       "      <td>0</td>\n",
       "      <td>comprendre métier achat comment organiser appe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>* Fendre du bois en forêt au merlin manuelleme...</td>\n",
       "      <td>0</td>\n",
       "      <td>fendre boi forêt merlin manuellemer débarder b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2 jours au CDI , 1 jour en arts plastiques , 1...</td>\n",
       "      <td>0</td>\n",
       "      <td>2 jour cdi 1 jour art plastique 1 jour musiqu ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4 jours au sein du Bureau des affaires institu...</td>\n",
       "      <td>1</td>\n",
       "      <td>4 jour sein bureau affaire institutionnel fina...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with fs.open(\"elissamim/text_classification_men/data/stages-votes.json\", \"r\") as file:\n",
    "    df = pd.read_json(file)\n",
    "\n",
    "df = df.groupby(\"phrase_text\", as_index = False)[\"sol\"].apply(lambda x: x.mode().iloc[0])\n",
    "df[\"sol\"]=df[\"sol\"].apply(lambda x: 1 if x == \"ok\" else 0)\n",
    "df[\"clean_phrase_text\"] = df[\"phrase_text\"].apply(lambda x: nltk_text_preprocessing(x, True))\n",
    "df = df[df[\"clean_phrase_text\"] != \"\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "071f8ae2-9587-4608-a156-ba67eb9896c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Static embeddings:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Classification algorithms:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Classification algorithms:  20%|██        | 1/5 [00:08<00:35,  8.85s/it]\u001b[A\n",
      "Classification algorithms:  40%|████      | 2/5 [00:44<01:13, 24.62s/it]\u001b[A\n",
      "Classification algorithms:  60%|██████    | 3/5 [01:11<00:51, 25.70s/it]\u001b[A\n",
      "Classification algorithms:  80%|████████  | 4/5 [01:16<00:17, 17.37s/it]\u001b[A\n",
      "Classification algorithms: 100%|██████████| 5/5 [02:15<00:00, 27.18s/it]\u001b[A\n",
      "Static embeddings:  33%|███▎      | 1/3 [02:15<04:31, 135.90s/it]\n",
      "Classification algorithms:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Classification algorithms:  20%|██        | 1/5 [00:05<00:23,  5.97s/it]\u001b[A\n",
      "Classification algorithms:  40%|████      | 2/5 [00:35<00:59, 19.95s/it]\u001b[A\n",
      "Classification algorithms:  60%|██████    | 3/5 [00:58<00:42, 21.11s/it]\u001b[A\n",
      "Classification algorithms:  80%|████████  | 4/5 [01:02<00:14, 14.47s/it]\u001b[A\n",
      "Classification algorithms: 100%|██████████| 5/5 [02:21<00:00, 28.29s/it]\u001b[A\n",
      "Static embeddings:  67%|██████▋   | 2/3 [04:37<02:19, 139.17s/it]\n",
      "Classification algorithms:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Classification algorithms:  20%|██        | 1/5 [00:05<00:22,  5.51s/it]\u001b[A\n",
      "Classification algorithms:  40%|████      | 2/5 [00:33<00:56, 18.86s/it]\u001b[A\n",
      "Classification algorithms:  60%|██████    | 3/5 [00:55<00:40, 20.03s/it]\u001b[A\n",
      "Classification algorithms:  80%|████████  | 4/5 [00:59<00:13, 13.74s/it]\u001b[A\n",
      "Classification algorithms: 100%|██████████| 5/5 [01:29<00:00, 17.97s/it]\u001b[A\n",
      "Static embeddings: 100%|██████████| 3/3 [06:07<00:00, 122.40s/it]\n"
     ]
    }
   ],
   "source": [
    "X = df[\"clean_phrase_text\"]\n",
    "y = df[\"sol\"]\n",
    "\n",
    "static_embedding_models = {\n",
    "    \"Bag of Words\":CountVectorizer(),\n",
    "    \"TF\":TfidfVectorizer(use_idf=False, norm = \"l1\"),\n",
    "    \"TF-IDF\":TfidfVectorizer(),\n",
    "    \"Word2Vec\": ,\n",
    "    \"FastText\": \n",
    "}\n",
    "\n",
    "classification_models = {\n",
    "    \"Logistic Regression\":LogisticRegression(),\n",
    "    \"Random Forest\":RandomForestClassifier(),\n",
    "    \"Linear SVM\":SVC(kernel=\"linear\", probability=True),\n",
    "    \"Multinomial Naive Bayes\":MultinomialNB(),\n",
    "    \"XGBoost\":XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "}\n",
    "\n",
    "dict_scores = {}\n",
    "\n",
    "for embedding_name, embedding_model in tqdm(static_embedding_models.items(),\n",
    "                                           desc=\"Static embeddings\"):\n",
    "\n",
    "    dict_scores[embedding_name] = {}\n",
    "    \n",
    "    for classification_name, classification_model in tqdm(classification_models.items(),\n",
    "                                                         desc=\"Classification algorithms\"):\n",
    "        \n",
    "        pipeline = Pipeline(\n",
    "            [\n",
    "                (\"feature_extraction\", embedding_model),\n",
    "                (\"classifier\", classification_model)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        scores = cross_val_score(pipeline, X, y, cv=10, scoring=\"accuracy\")\n",
    "\n",
    "        dict_scores[embedding_name][classification_name] = f\"{np.mean(scores):.3f} ± {np.std(scores):.3f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8527d47-bd23-43a4-8d50-7ba0327a0701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Bag of Words': {'Linear SVM': '0.609 ± 0.023',\n",
      "                  'Logistic Regression': '0.655 ± 0.029',\n",
      "                  'Multinomial Naive Bayes': '0.614 ± 0.052',\n",
      "                  'Random Forest': '0.633 ± 0.026',\n",
      "                  'XGBoost': '0.635 ± 0.023'},\n",
      " 'TF': {'Linear SVM': '0.605 ± 0.012',\n",
      "        'Logistic Regression': '0.604 ± 0.019',\n",
      "        'Multinomial Naive Bayes': '0.606 ± 0.004',\n",
      "        'Random Forest': '0.638 ± 0.023',\n",
      "        'XGBoost': '0.651 ± 0.040'},\n",
      " 'TF-IDF': {'Linear SVM': '0.629 ± 0.033',\n",
      "            'Logistic Regression': '0.659 ± 0.032',\n",
      "            'Multinomial Naive Bayes': '0.621 ± 0.024',\n",
      "            'Random Forest': '0.650 ± 0.027',\n",
      "            'XGBoost': '0.623 ± 0.036'}}\n"
     ]
    }
   ],
   "source": [
    "pprint(dict_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
