{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e6b8d3d-313d-4110-8e7e-c432b5dd6d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2d5f164-1887-4c04-86d3-8a23536e35de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr-core-news-sm==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import s3fs\n",
    "import sys\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import fireducks.pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import (\n",
    "                        CamembertTokenizer,\n",
    "                        CamembertForSequenceClassification,\n",
    "                        Trainer,\n",
    "                        TrainingArguments\n",
    "                        )\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "from ml_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c57e05e-5c63-47ba-b21b-7a111e2a3bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "warnings.simplefilter(\"ignore\")\n",
    "fs = s3fs.S3FileSystem(\n",
    "            client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"},\n",
    "            key=os.environ[\"Accesskey\"],\n",
    "            secret=os.environ[\"Secretkey\"],\n",
    "            token=os.environ[\"Token\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbcb4bb5-473b-48ef-af3c-cbe01f1ae3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase_text</th>\n",
       "      <th>sol</th>\n",
       "      <th>clean_phrase_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>* Aider à la mise en place de l évènement Shar...</td>\n",
       "      <td>0</td>\n",
       "      <td>aider mise place évènemer shareplan envoi rapp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>* Comprendre le métier des achats * Comment or...</td>\n",
       "      <td>0</td>\n",
       "      <td>comprendre métier achat comment organiser appe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>* Fendre du bois en forêt au merlin manuelleme...</td>\n",
       "      <td>0</td>\n",
       "      <td>fendre boi forêt merlin manuellemer débarder b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2 jours au CDI , 1 jour en arts plastiques , 1...</td>\n",
       "      <td>0</td>\n",
       "      <td>2 jour cdi 1 jour art plastique 1 jour musiqu ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4 jours au sein du Bureau des affaires institu...</td>\n",
       "      <td>1</td>\n",
       "      <td>4 jour sein bureau affaire institutionnel fina...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with fs.open(\"elissamim/text_classification_men/data/stages-votes.json\", \"r\") as file:\n",
    "    df = pd.read_json(file)\n",
    "\n",
    "df = df.groupby(\"phrase_text\", as_index = False)[\"sol\"].apply(lambda x: x.mode().iloc[0])\n",
    "df[\"sol\"]=df[\"sol\"].apply(lambda x: 1 if x == \"ok\" else 0)\n",
    "df[\"clean_phrase_text\"] = df[\"phrase_text\"].apply(lambda x: nltk_text_preprocessing(x, True))\n",
    "df = df[df[\"clean_phrase_text\"] != \"\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6815791-201f-4a35-84d8-f638af0ab70d",
   "metadata": {},
   "source": [
    "# Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "899692e0-0014-4bf4-9d69-049f04d7f460",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df[\"clean_phrase_text\"].tolist(),\n",
    "    df[\"sol\"].tolist(),\n",
    "    test_size=.2,\n",
    "    stratify=df[\"sol\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# We use CamemBERT Tokenizer (Byte-Level BPE) with truncation to 512 tokens and add [PAD] tokens for sequences that are too short\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\",\n",
    "                                              do_lower_case=True)\n",
    "train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=512)\n",
    "val_encodings = tokenizer(X_val, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# We define the dataset class for PyTorch model fine-tuning\n",
    "class ClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            key: torch.tensor(val[idx])\n",
    "            for key, val in self.encodings.items()\n",
    "        } | {\"labels\": torch.tensor(self.labels[idx])}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = ClassificationDataset(train_encodings, y_train)\n",
    "val_dataset = ClassificationDataset(val_encodings, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f55fc0-9db6-4e9c-90c0-8ca8fcf35e9e",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fb6229-f2d8-49c9-8568-e28b7617998e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[I 2025-05-01 16:29:44,362] A new study created in memory with name: no-name-1994bfd8-037c-4b25-8573-910cd664aa9a\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='102' max='102' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [102/102 00:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='102' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34/34 02:39]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 16:30:26,507] Trial 0 finished with value: 0.6804511278195489 and parameters: {'learning_rate': 1.5280571463102804e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 32, 'weight_decay': 0.11198410887080655, 'warmup_steps': 147}. Best is trial 0 with value: 0.6804511278195489.\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='268' max='268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [268/268 00:59, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 16:31:29,266] Trial 1 finished with value: 0.7030075187969925 and parameters: {'learning_rate': 1.079115015424602e-05, 'num_train_epochs': 4, 'per_device_train_batch_size': 16, 'weight_decay': 0.25810209532699663, 'warmup_steps': 293}. Best is trial 1 with value: 0.7030075187969925.\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='665' max='665' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [665/665 01:33, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.551600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 16:33:05,523] Trial 2 finished with value: 0.6954887218045113 and parameters: {'learning_rate': 3.090742533042018e-05, 'num_train_epochs': 5, 'per_device_train_batch_size': 8, 'weight_decay': 0.042243791459208535, 'warmup_steps': 85}. Best is trial 1 with value: 0.7030075187969925.\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='332' max='399' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [332/399 00:46 < 00:09, 7.09 it/s, Epoch 2.49/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    labels = p.label_ids\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "# Define the search space for Optuna\n",
    "def model_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 3, 5),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32]),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0, 0.3),\n",
    "        \"warmup_steps\": trial.suggest_int(\"warmup_steps\", 0, 500),\n",
    "    }\n",
    "\n",
    "# Define basic training args (used as a base)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    #evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",  # Disable saving for search runs\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=False,\n",
    ")\n",
    "\n",
    "# Use the Trainer to run hyperparameter search\n",
    "trainer = Trainer(\n",
    "    model_init=lambda: CamembertForSequenceClassification.from_pretrained(\"camembert-base\", num_labels=2),\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Run the search\n",
    "best_trial = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",              \n",
    "    hp_space=model_hp_space,\n",
    "    n_trials=10                       \n",
    ")\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7a5142-0f44-42ae-8a11-9487dbc9653b",
   "metadata": {},
   "source": [
    "# Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d280f060-fc2c-4b9d-8933-cc99eb320457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='399' max='399' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [399/399 3:00:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.669200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.666200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.641400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.671200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.653900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.657200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.693600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.707300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.675200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.643200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.716300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.692100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.647200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.641500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.623400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.646300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.635600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.645600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.638800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.651200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.593600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.573600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.650700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.544000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.574600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.553100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.560800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.553200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.554800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.646100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.533600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.559100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.534900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.479100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.568800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.520100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.616200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.500800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=399, training_loss=0.6134638523398187, metrics={'train_runtime': 10850.555, 'train_samples_per_second': 0.293, 'train_steps_per_second': 0.037, 'total_flos': 127464972991200.0, 'train_loss': 0.6134638523398187, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We load the classifier : CamemBERT Head with a classification head with the number of outputs for the final layer of 2\n",
    "# The loss is then by default CrossEntropyLoss()\n",
    "model = CamembertForSequenceClassification.from_pretrained(\"camembert-base\", \n",
    "                                                           num_labels=2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# We use the previous hyperparameters\n",
    "best_params = best_trial.hyperparameters\n",
    "\n",
    "# We define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    #evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    per_device_train_batch_size=best_params[\"per_device_train_batch_size\"],\n",
    "    num_train_epochs=best_params[\"num_train_epochs\"],\n",
    "    weight_decay=best_params[\"weight_decay\"],\n",
    "    warmup_steps=best_params[\"warmup_steps\"],\n",
    ")\n",
    "\n",
    "# We train the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "\n",
    "predictions = trainer.predict(val_dataset)\n",
    "pred_labels = predictions.predictions.argmax(axis=1)\n",
    "\n",
    "print(\"Predicted labels:\", pred_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
